{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buzz Words but What do They Mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NNs are a small subset of ai, ml, and deep learning](images\\what_is_a_neural_network\\small_subset_of_ai.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network\n",
    "\n",
    "![simple neural network architecture](images\\what_is_a_neural_network\\basic_neural_network.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are really a tiny subset of a bunch of different, larger categories of problem-solving techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![basic neuron in a neural network](images\\what_is_a_neural_network\\basic_neuron_with_bias.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic unit of a neural network is a single neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Parts of a Neuron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. inputs\n",
    "2. weight & bias\n",
    "3. sum (can be expressed as the dot product)\n",
    "4. activation function \n",
    "5. output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The basic Neuron class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNeuron:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def calc_neuron_output(self, inputs):\n",
    "        sum = 0\n",
    "        for inpt, weight in zip(inputs, self.weights):\n",
    "            sum += inpt * weight\n",
    "        sum += self.bias\n",
    "        return sum \n",
    "    \n",
    "        ###############\n",
    "        ## IMPORTANT ##\n",
    "        ###############\n",
    "        # there is a much better way to do this calculation, which we will talk about. This is just for instructional purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our basic Neuron class contains a few methods and a few attributes. \n",
    "\n",
    "It has a constructor which allows us to create Neurons.\n",
    "\n",
    "It has a calc_neuron_output function (function and method mean the same thing) which takes in some inputs and performs the following calculation: \n",
    "\n",
    "\\begin{equation*}\n",
    "y_j = b_j +  \\sum_{i} x_iw_{ij}\n",
    "\\end{equation*}\n",
    "\n",
    "Which means the output of neuron \"y sub j\" is the sum of all the neuron's inputs times their respective weights plus a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = BasicNeuron()\n",
    "n.weights = [0.1, 0.2, 0.3, 0.4]\n",
    "n.bias = 1\n",
    "inpts = [1, 1, 1, 1]\n",
    "\n",
    "n.calc_neuron_output(inpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our basic Neuron class works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting a layer together\n",
    "\n",
    "A \"layer\" is composed of many neurons, each with their own weights and biases.\n",
    "\n",
    "The benefit of thinking of our neural network in terms of layers is it simplifies a lot of the calculations we must do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![types of activation functions](images\\what_is_a_neural_network\\dot_product_representation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the dot product instead of calculting the output for each and every neuron makes things so much easier and more efficient. \n",
    "\n",
    "Actually, we don't even need our BasicNeuron class if we use the dot product instead of calculating the output of each neuron one-by-one. All we need to consider is the ENTIRE layer and all of the weights and biases that belong to it.\n",
    "\n",
    "Lets make things simpler by making a layer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, number_input_neurons = 0, number_output_neurons = 0, weights = np.array([]), biases = np.array([])):\n",
    "        self.number_input_neurons = number_input_neurons\n",
    "        self.number_output_neurons = number_output_neurons\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        \n",
    "    def initalize_random_weights(self):\n",
    "        self.weights = np.random.rand(self.number_output_neurons, self.number_input_neurons)\n",
    "    \n",
    "    def initalize_random_biases(self):\n",
    "        self.biases = np.random.rand(self.number_output_neurons, 1)\n",
    "        \n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.weights, self.input) + self.biases\n",
    "        return self.output\n",
    "    \n",
    "    # We'll explain this in a bit! For now know that this is important for later.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:\n",
      "[[0.4359949  0.02592623]\n",
      " [0.54966248 0.43532239]]\n",
      "\n",
      "##########\n",
      "\n",
      "baises:\n",
      "[[0.4203678 ]\n",
      " [0.33033482]]\n"
     ]
    }
   ],
   "source": [
    "layer = Layer(2, 2)\n",
    "layer.initalize_random_weights()\n",
    "layer.initalize_random_biases()\n",
    "print(\"weights:\\n{}\".format(layer.weights))\n",
    "print(\"\\n##########\\n\")\n",
    "print(\"baises:\\n{}\".format(layer.biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4203678 ]\n",
      " [0.33033482]]\n",
      "[[0.44629403]\n",
      " [0.76565721]]\n",
      "[[0.8563627]\n",
      " [0.8799973]]\n",
      "[[0.88228894]\n",
      " [1.31531969]]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[0,0],\n",
    "                   [0,1],\n",
    "                   [1,0],\n",
    "                   [1,1]])\n",
    "\n",
    "expected_outputs = np.array([[0], [1], [1], [0]])\n",
    "inputs = np.reshape(inputs, (4,-1,1))\n",
    " # rotates the entire array to be shaped like \n",
    "     #[[[0],\n",
    "     # [0]],\n",
    "     #\n",
    "     #[[0],\n",
    "     #[1]],\n",
    "     #\n",
    "     #[[1],\n",
    "     #[0]],\n",
    "     #\n",
    "     #[[1],\n",
    "     #[1]]]\n",
    "\n",
    "for inpt in inputs:\n",
    "    print(layer.forward_propagation(inpt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![types of activation functions](images\\what_is_a_neural_network\\types_of_activation_functions.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sigmoid Activation Function\n",
    "This is what we'll be using for this simple example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![the sigmoid function](images\\what_is_a_neural_network\\sigmoid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "# we'll need the derivative of this function later!\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.88228894]\n",
      " [1.31531969]]\n",
      "\n",
      "#######\n",
      "\n",
      "after applying the sigmoid function to every value in the array:\n",
      "\n",
      "[[0.70729632]\n",
      " [0.78840197]]\n"
     ]
    }
   ],
   "source": [
    "print(layer.output)\n",
    "print(\"\\n#######\\n\")\n",
    "print(\"after applying the sigmoid function to every value in the array:\\n\\n{}\".format(sigmoid(layer.output)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works just about how you would expect it to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a basic Layer class which takes some inputs, has some weights and biases, and helps us train our neural network. \n",
    "\n",
    "You might be wondering what it means to \"train\" a NN, and that is a very important question. \n",
    "\n",
    "A neural network works because we can use a little bit of calculus to adjust the weights and biases in a smart way that allows us to make our predictions more accurate. This process of adjusting the weights and biases is what it means to \"train\" a NN. It is ok if you are not super comfortable with the calculus, it is not super important to understand fully right now, but it might give you a better intuition into exactly what makes neural networks work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
